{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Hands-On Python Natural Language Processing By Aman Kedia and Mayank Rasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tokens with .split\n",
    "sentence = \"The capital of China is Beijing\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization issues with apostrophes\n",
    "sentence = \"Beijing is where we'll go\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization with a period\n",
    "sentence = \"A friend is pursuing his M.S from Beijing\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$3000.0',\n",
       " '-',\n",
       " '$8000.0',\n",
       " 'in',\n",
       " 'USA',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regular expression based tokenizeers\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\w+|\\$[\\d\\.]+|\\S+ regular expression allows three alternative patterns:\n",
    "First alternative: \\w+ that matches any word character (equal to [a-zA-Z0-9_]). The + is a quantifier and matches between one and unlimited times as many times as possible.\n",
    "\n",
    "Second alternative: \\$[\\d\\.]+. Here, \\$ matches the character $, \\d matches a digit between 0 and 9, \\. matches the character . (period), and + again acts as a quantifier matching between one and unlimited times.\n",
    "\n",
    "Third alternative: \\S+. Here, \\S accepts any non-whitespace character and + again acts the same way as in the preceding two alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3000.0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Treebank tokenizer also uses regular expressions to tokenize text according to the\n",
    "#Penn TreebankHere, words are mostly split based on punctuation.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@amankedia',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxxxxxxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet Tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Strip and presrve\n",
    "#The parameter strip_handles, when set to True, removes the handles mentioned in a post/tweet. \n",
    "#preserve_case, which, when set to False, converts everything to lower case in order to normalize the vocabulary.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "#Snowball languages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter stemmer works only with strings, whereas the Snowball stemmer works with both strings and Unicode data. The Snowball stemmer also allows the option to ignore stopwords as an inherent functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned',\n",
    "'humbled', 'sized', 'meeting', 'stating',\n",
    " 'siezing', 'itemization', 'traditional', 'reference', 'colonizer',\n",
    "'plotted', 'having', 'generously']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
     ]
    }
   ],
   "source": [
    "#Porter Stem\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
     ]
    }
   ],
   "source": [
    "#Snowball Stemmer\n",
    "stemmer2 = SnowballStemmer(language='english')\n",
    "singles = [stemmer2.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emanuel.s/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "s = \"We are putting in efforts to enhance our understanding of \\\n",
    "        Lemmatization\"\n",
    "token_list = s.split()\n",
    "print(\"The tokens are: \", token_list)\n",
    "\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token \\\n",
    "                              in token_list])\n",
    "print(\"The lemmatized output is: \", lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('putting', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('enhance', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('understanding', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Lemmatization', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using POS to help Lemmatizer\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
    "\"\"\"Maps POS tags to first character lemmatize() accepts. \n",
    "We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "def get_part_of_speech_tags(token):\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We be put in effort to enhance our understand of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token,\n",
    "get_part_of_speech_tags(token)) for token in token_list]\n",
    "print(' '.join(lemmatized_output_with_POS_information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are put in effort to enhanc our understand of lemmat\n"
     ]
    }
   ],
   "source": [
    "#Compare to snowball stemmer\n",
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emanuel.s/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"than, off, whom, aren't, an, mightn, to, weren't, couldn't, themselves, you, isn, both, mustn, nor, don, here, has, you'll, against, when, doesn, all, him, above, just, each, my, not, she, you've, up, can, any, now, only, and, d, himself, shouldn, through, isn't, your, were, ma, where, didn't, herself, mightn't, during, of, in, does, will, who, after, needn't, aren, wouldn, the, how, into, doing, having, she's, do, it's, this, between, own, yourselves, out, theirs, so, other, few, some, before, wouldn't, weren, t, its, if, it, should, about, most, on, for, his, her, should've, y, shouldn't, you're, couldn, won, i, are, over, wasn't, such, was, why, needn, under, shan't, you'd, ll, doesn't, ain, while, myself, below, shan, be, down, which, at, ours, them, did, these, our, they, hers, o, that'll, is, further, me, wasn, there, with, hasn, by, then, very, he, we, hadn't, yourself, what, but, or, am, because, no, as, didn, haven, don't, that, itself, mustn't, once, yours, ourselves, being, had, hadn, a, again, have, hasn't, until, same, m, haven't, their, more, won't, those, ve, re, been, from, too, s\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping Wh- words\n",
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding Lemmatization'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
    "for word in wh_words:\n",
    "    stop.remove(word)\n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
    "\" \".join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language',\n",
       " 'Language Processing',\n",
       " 'Processing is',\n",
       " 'is the',\n",
       " 'the way',\n",
       " 'way to',\n",
       " 'to go']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bigrams\n",
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[\" \".join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Language Processing is',\n",
       " 'Processing is the',\n",
       " 'is the way',\n",
       " 'the way to',\n",
       " 'way to go']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trigrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "[\" \".join(token) for token in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My First HeadingMy first paragraph.\n"
     ]
    }
   ],
   "source": [
    "#Using Beautiful soup to help clear HTML Tags\n",
    "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html)\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = set(s.split())\n",
    "vocabulary = sorted(tokens)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3 Vocab\n",
    "\n",
    "In simple terms, a lexicon can be thought of as a dictionary of terms that are called lexemes.\n",
    "\n",
    "Phonemes can be thought of as the speech sounds, made by the mouth or unit of sound, that can differentiate one word from another in a language.\n",
    "\n",
    "Graphemes are groups of letters of size one or more that can represent these individual sounds or phonemes. The word spoon consists of five letters that actually represent four phonemes, identified by the graphemes s, p, oo,  and n.\n",
    "\n",
    "A morpheme is the smallest meaningful unit in a language. The word unbreakable is composed of three morphemes:\n",
    "un—a bound morpheme signifying not break—the root morpheme able—a free morpheme signifying can be done\n",
    "\n",
    "Tokenization - In order to build up a vocabulary, the first thing to do is to break the documents or sentences into chunks called tokens.\n",
    "\n",
    "Regular expressions are sequences of characters that define a search pattern. \n",
    "\n",
    "Stemming - a crude attempt is made to remove the inflectional forms of a word and bring them to a base form called the stem\n",
    "\n",
    "Overstemming - words that are stemmed to the same root should have been stemmed to different roots\n",
    "\n",
    "Understemming - words that should have been stemmed to the same root aren't stemmed to it\n",
    "\n",
    "Lemmatization is a process wherein the context is used to convert a word to its meaningful base form.\n",
    "\n",
    "Case Folding - turns all letters in text corpus to lower case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
